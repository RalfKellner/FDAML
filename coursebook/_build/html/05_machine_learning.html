
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Machine learning &#8212; Financial Data Analytics and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05_machine_learning';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Multivariate analysis - dependence matters!" href="04_dependence_matters.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="00_welcome.html">
  
  
  
  
  
  
    <p class="title logo__title">Financial Data Analytics and Machine Learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_welcome.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_introduction.html">Financial markets</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_empirical_asset_returns.html">Empirical analysis of asset returns</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_univariate_models.html">Modeling univariate asset return distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_dependence_matters.html">Multivariate analysis - dependence matters!</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Machine learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/05_machine_learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-between-machine-learning-and-traditional-statistical-analysis">Differences Between Machine Learning and Traditional Statistical Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-tasks">Machine learning tasks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">Supervised learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression">Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification">Binary classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-classification">Multi-classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-and-generalization-of-supervised-learning-models">Validation and generalization of supervised learning models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning">Unsupervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis">Principal component analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering">K-means clustering</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="machine-learning">
<h1>Machine learning<a class="headerlink" href="#machine-learning" title="Link to this heading">#</a></h1>
<p>So far, we analyzed financial data with an emphasis on stock market data and a few of its fundamental empirical characteristics and theories. For this purpose, we utilized statistical analysis including estimators and a few statistical models such as linear and auto-regressive regression. Due to the increasing success and importance of machine learning algorithms in various fields, they also gained more and more interest in the financial domain. While some of them may improve the understanding of financial characteristics, these algorithms also experience limitations which do not exist in other disciplines. An example is the limited ability to predict future developments of asset prices which is given due to the efficient and immediate processing of new information on financial markets.</p>
<section id="differences-between-machine-learning-and-traditional-statistical-analysis">
<h2>Differences Between Machine Learning and Traditional Statistical Analysis<a class="headerlink" href="#differences-between-machine-learning-and-traditional-statistical-analysis" title="Link to this heading">#</a></h2>
<p>Machine learning (ML) and traditional statistical analysis share common ground in their use of data to make predictions and uncover patterns. However, there are several key differences between the two approaches, particularly in their focus, methodology, and objectives.</p>
<ul class="simple">
<li><p>Focus and Objectives</p>
<ul>
<li><p>Machine Learning:</p>
<ul>
<li><p>Focus: The primary goal is to build models that can make accurate predictions or classifications based on input data. The emphasis is on the performance of the model on unseen data.</p></li>
<li><p>Objectives: Machine learning aims to maximize predictive accuracy, often through iterative training and testing, with less concern about the interpretability of individual variables.</p></li>
</ul>
</li>
<li><p>Traditional Statistical Analysis:</p>
<ul>
<li><p>Focus: The main goal is to understand the relationships between variables and to test hypotheses about these relationships. There is a strong emphasis on the statistical significance of variables.</p></li>
<li><p>Objectives: Traditional statistical analysis seeks to infer the underlying relationships within the data, providing insights into causality and the significance of predictors.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Methodology</p>
<ul>
<li><p>Machine Learning:</p>
<ul>
<li><p>Data Handling: ML models can handle large, high-dimensional datasets and are capable of processing unstructured data (e.g., images, text).</p></li>
<li><p>Model Training: Emphasizes training models on a portion of the data and validating performance on separate test sets. Techniques such as cross-validation are commonly used.</p></li>
<li><p>Algorithms: Utilizes a wide range of algorithms, including decision trees, support vector machines, neural networks, and ensemble methods, which often require substantial computational resources.
Evaluation: Performance is evaluated based on metrics like accuracy, precision, recall, F1 score, and area under the ROC curve (AUC), focusing on how well the model generalizes to new data.</p></li>
</ul>
</li>
<li><p>Traditional Statistical Analysis:</p>
<ul>
<li><p>Data Handling: Often deals with smaller datasets and structured data, with a focus on ensuring data meets assumptions required by statistical models (e.g., normality, homoscedasticity).</p></li>
<li><p>Model Training: Involves fitting models to the entire dataset, with an emphasis on parameter estimation and hypothesis testing.</p></li>
<li><p>Algorithms: Commonly uses linear models (e.g., linear regression, logistic regression), generalized linear models, and time series analysis, which are computationally less intensive.
Evaluation: Focuses on the significance of individual variables using p-values, confidence intervals, and R-squared values to assess the fit of the model.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Interpretability and Significance</p>
<ul>
<li><p>Machine Learning:</p>
<ul>
<li><p>Interpretability: Often, ML models, especially complex ones like deep neural networks, are seen as “black boxes” with less emphasis on the interpretability of individual parameters.</p></li>
<li><p>Variable Importance: Techniques such as feature importance scores, SHAP values, and partial dependence plots can be used to interpret the influence of variables, but these are typically secondary to predictive performance.</p></li>
</ul>
</li>
<li><p>Traditional Statistical Analysis:</p>
<ul>
<li><p>Interpretability: High priority is given to understanding and interpreting the coefficients of the model. The relationships between variables and outcomes are explicitly described.</p></li>
<li><p>Statistical Significance: Emphasis on determining the statistical significance of predictors using tests like t-tests for coefficients, ensuring that the relationships are not due to random chance.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Use Cases</p>
<ul>
<li><p>Machine Learning:</p>
<ul>
<li><p>Applications: Well-suited for tasks requiring high predictive accuracy, such as image and speech recognition, recommendation systems, and fraud detection.</p></li>
<li><p>Adaptability: Capable of handling complex, non-linear relationships and large-scale data, making it adaptable to various domains.</p></li>
</ul>
</li>
<li><p>Traditional Statistical Analysis:</p>
<ul>
<li><p>Applications: Ideal for scenarios where understanding the relationship between variables is crucial, such as in experimental studies, clinical trials, and econometric analyses.</p></li>
<li><p>Rigorous Testing: Provides rigorous testing of hypotheses and clearer insights into causal relationships, which is essential in scientific research.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>While both machine learning and traditional statistical analysis leverage data to derive insights and make predictions, they differ significantly in their focus, methodology, and objectives. Machine learning prioritizes predictive accuracy and handling complex, large-scale data, often at the expense of interpretability. Traditional statistical analysis, on the other hand, emphasizes understanding the relationships between variables and the statistical significance of these relationships, providing clear, interpretable models. Each approach has its strengths and is suited to different types of problems and contexts.</p>
</section>
<section id="machine-learning-tasks">
<h2>Machine learning tasks<a class="headerlink" href="#machine-learning-tasks" title="Link to this heading">#</a></h2>
<p>In this chapter, we are going to study a few fundamental algorithms of machine learning. Where appropriate, we compare the use of models for machine learning and for statistical analysis. Afterwards, we examine applications of machine learning methods in the field of finance.</p>
<p>Machine Learning is a branch of artificial intelligence that focuses on developing algorithms and statistical models that enable computers to perform tasks without explicit instructions. Instead of following a pre-defined program, these systems learn from data and improve their performance over time through experience. This ability to learn and adapt makes machine learning a powerful tool for solving complex problems in various fields, from healthcare to finance to robotics.
Definition of Machine Learning</p>
<p>Machine learning is defined as the study of computer algorithms that improve automatically through experience and the use of data. It involves the development of models that can recognize patterns and make decisions with minimal human intervention. The process typically involves training a model on a dataset, validating its performance, and then applying it to new data for prediction or classification.</p>
<p>Traditionally, the most three important sub-fields of machine learning are:</p>
<ul class="simple">
<li><p>Supervised learning: In supervised learning, the model is trained on a labeled dataset, which means each training example is paired with an output label. The algorithm learns to map inputs to outputs based on this labeled data. Sometimes the labeling process simply includes to define a target variable, sometime the labeling process needs to be done by humans which usually is time-consuming and costly. Examples in the financial domain are credit default prediction, earning forecasts or macroeconomic forecasting.</p></li>
<li><p>Unsupervised learning: Unsupervised learning involves training a model on data without labeled responses. The goal is to identify hidden patterns or intrinsic structures in the input data. Examples in the financial domain are relative company valuation by means of clustering or statistical factor modeling.</p></li>
<li><p>Reinforcement learning: In reinforcement learning, an agent interacts with an environment and learns to perform actions to maximize cumulative rewards in the future. Examples in the financial domain are hedging or order processing for asset trading.</p></li>
</ul>
</section>
<section id="supervised-learning">
<h2>Supervised learning<a class="headerlink" href="#supervised-learning" title="Link to this heading">#</a></h2>
<p>Supervised learning may further be split into regression and classification tasks, whereby, the latter can be either binary, multi-class or multi-label classification. We are going to study fundamental algorithms for both tasks in this sub-chapter. While a plethora of supervised learning algorithms exist, I think the analysis of these baseline algorithms provides a good understanding how these and other algorithms work.</p>
<p>In general, the overall goal of an supervised learning algorithm is to identify a functional relationship between feature variables <span class="math notranslate nohighlight">\(\boldsymbol{x} \)</span> and the target variable <span class="math notranslate nohighlight">\(y\)</span>. The functional relationship is unknown and must be approximated by a parametric model which is calibrated by a set of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. Deviations from the functional relationship are assumed to be random and are usually denoted by <span class="math notranslate nohighlight">\(\epsilon\)</span>. Putting this together, we have:</p>
<div class="math notranslate nohighlight">
\[
y = f_{\boldsymbol{\theta}}\left(\boldsymbol{x}\right) + \epsilon
\]</div>
<p>Together with <span class="math notranslate nohighlight">\(\epsilon\)</span> this defines the full model for <span class="math notranslate nohighlight">\(y\)</span>. The predictions themselves are made by:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f_{\boldsymbol{\theta}}\left(\boldsymbol{x}\right)
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is common to use <span class="math notranslate nohighlight">\(\beta\)</span> for parameters of regression models, we denote all parameters with <span class="math notranslate nohighlight">\(\theta\)</span> for the description of the machine learning models. Furthermore, while variables are often called independent or dependent variables in statistical analysis, they are named feature and target variables in the machine learning domain.</p>
</div>
<section id="regression">
<h3>Regression<a class="headerlink" href="#regression" title="Link to this heading">#</a></h3>
<p>For regression problems, the target variable is on a metric scale. The linear regression model may be considered as one of the most popular regression models. While it is rather limited in its ability to capture complex relationships, it is easy to estimate and creates predictions which can be interpreted with a large degree of transparency.</p>
<p><strong>Prediction</strong></p>
<p>When using linear regression, the aim is to predict values for <span class="math notranslate nohighlight">\(y\)</span>, given a linear relationship between <span class="math notranslate nohighlight">\(y\)</span> and features <span class="math notranslate nohighlight">\(\boldsymbol{x} = (x_{1}, ..., x_{n})\)</span>. The regression line is given by:</p>
<div class="math notranslate nohighlight">
\[
f_{\boldsymbol{\theta}}(\boldsymbol{x}) = \hat{y} = \boldsymbol{\theta}^T \boldsymbol{x} = \theta_1 \cdot x_{1} + ... + \theta_n \cdot x_{n} + \theta_0
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> need to be estimated.</p>
<p><strong>Model estimation</strong></p>
<p>This is usually done by minimizing squared deviations between predictions and observed values for the target variable. I.e., the function to be minimized by calibrating the model parameters is:</p>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{\theta}) = \sum_{i = 1}^m \left( y_i - \boldsymbol{\theta}^T \boldsymbol{x}_i \right)^2
\]</div>
<p>where, <span class="math notranslate nohighlight">\(m\)</span> is the number of samples in the data set. This function is often called the <strong>loss function</strong> or the <strong>cost function</strong>. The optimization problem can be solved analytically. This means a closed form solution exists that returns parameter estimates if we insert observed data into a formula. For the linear regression, the estimator for the parameters of the regression line is:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\theta}} = \left( X^T X \right)^{-1} X^T \boldsymbol{y}
\]</div>
<p>where, <span class="math notranslate nohighlight">\(X\)</span> is the matrix with feature variables of the data sample and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> is the vector with realizations of the target variable.  In general, closed form solution do not exist for the majority of supervised learning algorithms, however, optimization which provide numerical solutions are implemented within popular python packages.</p>
<p>The linear regression model is only captures linear relationships adequately, and, thus, is often a biased estimator in the presence of non-linear relationships between feature and target variables. However, an advantage is its simple interpretation. If the feature variable is numerical, positive values <span class="math notranslate nohighlight">\(\theta_j &gt; 0\)</span> indicate a positive linear relationship and <span class="math notranslate nohighlight">\(\theta_j\)</span> quantifies the change in the target variable if <span class="math notranslate nohighlight">\(x_j\)</span> increases by one unit. In analogy, values for <span class="math notranslate nohighlight">\(\theta_j &lt; 0\)</span> can be interpreted. If all feature variables are within the same numerical range (by, e.g., standardization of numerical feature variables before parameter estimation), the absolute value for <span class="math notranslate nohighlight">\(\theta_j\)</span> can be interpreted as the feature importance of <span class="math notranslate nohighlight">\(x_j\)</span> for <span class="math notranslate nohighlight">\(y\)</span>. If feature variables are categorical, one usually transfers them by dummy or one-hot encoding.</p>
<p>If a variable has <span class="math notranslate nohighlight">\(K\)</span> categories, it is transformed to <span class="math notranslate nohighlight">\(K-1\)</span> or <span class="math notranslate nohighlight">\(K\)</span> binary <span class="math notranslate nohighlight">\(0/1\)</span> variables for its dummy or one-hot encoded representation, respectively. the binary variable is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x^{(k)} = 
\begin{cases}
1 &amp; \text{if } x = k \\
0 &amp; \text{else}
\end{cases}
\end{split}\]</div>
<p>For instance, when using dummy encoding, the data set:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Observation</p></th>
<th class="head"><p>Sentiment</p></th>
<th class="head"><p>Target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Positive</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Negative</p></td>
<td><p>-10</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Positive</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>Negative</p></td>
<td><p>-8</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Positive</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>Negative</p></td>
<td><p>-7</p></td>
</tr>
</tbody>
</table>
<p>If we create a dummy encoded version for the “Sentiment” feature:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Observation</p></th>
<th class="head"><p>Sentiment</p></th>
<th class="head"><p>Target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>0</p></td>
<td><p>-10</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>-8</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>1</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>0</p></td>
<td><p>-7</p></td>
</tr>
</tbody>
</table>
<p>And we create a dummy encoded version for the “Sentiment” feature:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Observation</p></th>
<th class="head"><p>Sentiment</p></th>
<th class="head"><p>Target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>The regression line for the dummy encoding is:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \theta_0 + \theta_1 x^{\text{positive}}
\]</div>
<p>The regression line for the one-hot encoding is:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \theta + \theta_1 x^{\text{positive}} + \theta_2 x^{\text{negative}}
\]</div>
<p>The choice is yours as your findings are consistent no matter which version is used. However, the interpretation differs. For the dummy encoded model <span class="math notranslate nohighlight">\(\theta_0\)</span> is the prediction for the target variable if the sentiment is negative and <span class="math notranslate nohighlight">\(\theta_0 + \theta_1\)</span> is the prediction for observations with positive sentiment. For the dummy encoded version, <span class="math notranslate nohighlight">\(\theta_0 + \theta_1\)</span> is the prediction for positive sentiment and <span class="math notranslate nohighlight">\(\theta_0 + \theta_2\)</span> for negative sentiment. Thus <span class="math notranslate nohighlight">\(\theta_1\)</span> is the deviation from the baseline <span class="math notranslate nohighlight">\(\theta_0\)</span> for observations with positive sentiment and <span class="math notranslate nohighlight">\(\theta_2\)</span> the deviation for observations with negative sentiment.</p>
<p><strong>Model evaluation</strong></p>
<p>Once a model is estimated, we want to assess its performance by means of metrics which need to be selected w.r.t. to the task at hand. For regression type problems, the goal is that predicted values should be close to actual observations. This makes the mean absolute deviation (<span class="math notranslate nohighlight">\(MAD\)</span>) and mean squared error (<span class="math notranslate nohighlight">\(MSE\)</span>) natural candidates:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
MAD = \frac{1}{m} \sum_{i=1}^m |y_i - \hat{y}_i| \\
MSE = \frac{1}{m} \sum_{i=1}^m \left( y_i - \hat{y}_i \right)^2
\end{split}\]</div>
<p>However, values for <span class="math notranslate nohighlight">\(MAE, MSE\)</span> are only meaningful if they can be compared to other models. Thus, it is reasonable to include a benchmark in a performance metric. An example is the coefficient of determination <span class="math notranslate nohighlight">\(R^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
R^2 = 1 - \frac{\sum_i \left( y_i - \hat{y}_i \right)^2}{\sum_i \left( y_i - \bar{y} \right)^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\({\bar{y}}\)</span> is the average of target variable realizations. If a model which uses feature variables for prediction (<span class="math notranslate nohighlight">\(\hat{y}_i\)</span>) generates better predictions than a model which does not use the information of feature variables (<span class="math notranslate nohighlight">\(\bar{y}\)</span>), the sum of <span class="math notranslate nohighlight">\(\left( y_i - \hat{y}_i \right)^2\)</span> should be smaller than the sum of <span class="math notranslate nohighlight">\(\left( y_i - \bar{y} \right)^2\)</span>. Thus the fraction in the formula is smaller than one. The better the model the more converges the fraction towards <span class="math notranslate nohighlight">\(0\)</span>, thus values close to <span class="math notranslate nohighlight">\(1\)</span> would speak for a high performance of the model which uses feature information.</p>
<p>An important misunderstanding in this context is the believe that models with values that are not close to <span class="math notranslate nohighlight">\(1\)</span> exhibit a bad performance. This is only true if the relationship between feature variables and the target variable is fully deterministic and not also impacted by randomness in the form of <span class="math notranslate nohighlight">\(\epsilon\)</span>. For instance, if a relationship is plagued by a lot of noise, values in the range of around 5% may already be seen as a significant improvement regarding the predictive performance of the model.</p>
<p><strong>Model assumptions</strong></p>
<p>The linear regression is one of the most applied and most popular models for statistical analysis. This is why its model assumptions and the failure to meet have been intensively studied. Depending if we want to use the linear regression model for the purpose of statistical analysis or prediction, only, the importance of failed assumptions and the way how to handle them differs. The most important model assumptions of the linear regression model are:</p>
<ul class="simple">
<li><p>Linear relationship between the independent and the dependent variable.</p></li>
<li><p>Normal distribution of the dependent variable</p></li>
<li><p>Homoscedasticity</p></li>
<li><p>Independent error terms</p></li>
<li><p>Low correlation of the independent variables (low or no multicollinearity)</p></li>
</ul>
<p>In short, if assumptions do not hold, the variance for the parameter estimator is usually mis-specified, mostly being underestimated. However, the estimator is usually asymptotically consistent. This means, parameter estimates likely do not systematically under- or overestimate the impact of feature variables. However, if the estimator’s standard error is falsely quantified, statistical inference regarding the impact of feature variables may be falsely assessed. Depending on the assumption this can be corrected, however, this is not discussed in more detail for this course.</p>
<p>Below, we see the output for a regression which regresses Apple’s daily returns on the three Fama-French risk factors. The method from the <em>statsmodels</em> package is used which has a greater emphasis on statistical analysis. The output includes the <span class="math notranslate nohighlight">\(R^2\)</span> value, parameter estimates and a few statistical tests.</p>
<ul class="simple">
<li><p>F-test:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_0: \theta_1 = ... = \theta_n = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \theta_j \neq 0\)</span>, at least for one <span class="math notranslate nohighlight">\(j\)</span></p></li>
</ul>
</li>
<li><p>t-test:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_0: \theta_j = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \theta_j \neq 0\)</span></p></li>
</ul>
</li>
<li><p>Omnibus and Jarque-Bera:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_0: \text{Residuals are normally distributed}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \text{Residuals are not normally distributed}\)</span></p></li>
</ul>
</li>
<li><p>Durbin Watson:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_0: \text{There is no autocorrelation in the residuals}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \text{There is autocorrelation in the residuals}\)</span></p></li>
</ul>
</li>
</ul>
<p>The latter is not rejected for values around <span class="math notranslate nohighlight">\(2\)</span>.</p>
<p>With the output below, we find that the three factors seem to be capable for explaining the variation of Apple’s returns (due to a good value for <span class="math notranslate nohighlight">\(R^2\)</span>). The distribution of Apple’s returns seems not to be normally distributed (p-values around zero for the Jarque-Bera and Omnibus test). Autocorrelation for the residuals seems to be moderate (as the Durbin-Watson test statistic is not that far away from <span class="math notranslate nohighlight">\(2\)</span>). <span class="math notranslate nohighlight">\(\theta_0, \theta_{SMB}\)</span> are not statistically different from zero for a significance level of 5%. However, Apple’s returns behave similar with the market and like a growth stock (due to the negative sign of <span class="math notranslate nohighlight">\(\theta_{HML}\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/sp500_1y.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="s2">&quot;Unnamed: 0&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="o">-</span><span class="mi">4</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;AAPL&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;RF&quot;</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.418
Model:                            OLS   Adj. R-squared:                  0.410
Method:                 Least Squares   F-statistic:                     51.87
Date:                Thu, 06 Jun 2024   Prob (F-statistic):           2.57e-25
Time:                        11:31:09   Log-Likelihood:                 725.28
No. Observations:                 221   AIC:                            -1443.
Df Residuals:                     217   BIC:                            -1429.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0009      0.001     -1.508      0.133      -0.002       0.000
Mkt-RF         0.9243      0.088     10.491      0.000       0.751       1.098
SMB           -0.0244      0.095     -0.256      0.798      -0.213       0.164
HML           -0.4845      0.101     -4.808      0.000      -0.683      -0.286
==============================================================================
Omnibus:                       45.805   Durbin-Watson:                   1.685
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.299
Skew:                          -0.962   Prob(JB):                     3.71e-23
Kurtosis:                       5.742   Cond. No.                         200.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>As the model assumption of a normal distribution is violated and we have an indication of (positive) autocorrelation, we can and should estimate standard errors with the Heteroskedasticity and Autocorrelation Consistent - HAC estimator. This is done in the cell below. We observe that the p-values for individual coefficient estimates change. However, they would not impact the statistical inference for this scenario. Of course, this can be different for other analyses.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;HC3&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.418
Model:                            OLS   Adj. R-squared:                  0.410
Method:                 Least Squares   F-statistic:                     51.92
Date:                Thu, 06 Jun 2024   Prob (F-statistic):           2.47e-25
Time:                        11:31:09   Log-Likelihood:                 725.28
No. Observations:                 221   AIC:                            -1443.
Df Residuals:                     217   BIC:                            -1429.
Df Model:                           3                                         
Covariance Type:                  HC3                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0009      0.001     -1.543      0.123      -0.002       0.000
Mkt-RF         0.9243      0.090     10.270      0.000       0.748       1.101
SMB           -0.0244      0.101     -0.241      0.810      -0.223       0.174
HML           -0.4845      0.106     -4.556      0.000      -0.693      -0.276
==============================================================================
Omnibus:                       45.805   Durbin-Watson:                   1.685
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.299
Skew:                          -0.962   Prob(JB):                     3.71e-23
Kurtosis:                       5.742   Cond. No.                         200.
==============================================================================

Notes:
[1] Standard Errors are heteroscedasticity robust (HC3)
</pre></div>
</div>
</div>
</div>
<p><strong>Regression example</strong></p>
<p>Below you can observe a popular toy dataset which is called <em>California Housing</em>. It is available via the sklearn datasets module and described as follows:</p>
<p>This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</p>
<p>A household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surprisingly large values for block groups with few households and many empty houses, such as vacation resorts.</p>
<p>The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000). Below you can take a look at the data set and some descriptive statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">cf_housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">cf_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cf_housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">cf_housing</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">cf_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">cf_housing</span><span class="o">.</span><span class="n">target_names</span><span class="p">]</span> <span class="o">=</span> <span class="n">cf_housing</span><span class="o">.</span><span class="n">target</span>
<span class="n">cf_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
      <th>MedHouseVal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
      <td>4.526</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
      <td>3.585</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
      <td>3.521</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
      <td>3.413</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
      <td>3.422</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cf_df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 9 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   MedInc       20640 non-null  float64
 1   HouseAge     20640 non-null  float64
 2   AveRooms     20640 non-null  float64
 3   AveBedrms    20640 non-null  float64
 4   Population   20640 non-null  float64
 5   AveOccup     20640 non-null  float64
 6   Latitude     20640 non-null  float64
 7   Longitude    20640 non-null  float64
 8   MedHouseVal  20640 non-null  float64
dtypes: float64(9)
memory usage: 1.4 MB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cf_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
      <th>MedHouseVal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.870671</td>
      <td>28.639486</td>
      <td>5.429000</td>
      <td>1.096675</td>
      <td>1425.476744</td>
      <td>3.070655</td>
      <td>35.631861</td>
      <td>-119.569704</td>
      <td>2.068558</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.899822</td>
      <td>12.585558</td>
      <td>2.474173</td>
      <td>0.473911</td>
      <td>1132.462122</td>
      <td>10.386050</td>
      <td>2.135952</td>
      <td>2.003532</td>
      <td>1.153956</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.499900</td>
      <td>1.000000</td>
      <td>0.846154</td>
      <td>0.333333</td>
      <td>3.000000</td>
      <td>0.692308</td>
      <td>32.540000</td>
      <td>-124.350000</td>
      <td>0.149990</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.563400</td>
      <td>18.000000</td>
      <td>4.440716</td>
      <td>1.006079</td>
      <td>787.000000</td>
      <td>2.429741</td>
      <td>33.930000</td>
      <td>-121.800000</td>
      <td>1.196000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.534800</td>
      <td>29.000000</td>
      <td>5.229129</td>
      <td>1.048780</td>
      <td>1166.000000</td>
      <td>2.818116</td>
      <td>34.260000</td>
      <td>-118.490000</td>
      <td>1.797000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.743250</td>
      <td>37.000000</td>
      <td>6.052381</td>
      <td>1.099526</td>
      <td>1725.000000</td>
      <td>3.282261</td>
      <td>37.710000</td>
      <td>-118.010000</td>
      <td>2.647250</td>
    </tr>
    <tr>
      <th>max</th>
      <td>15.000100</td>
      <td>52.000000</td>
      <td>141.909091</td>
      <td>34.066667</td>
      <td>35682.000000</td>
      <td>1243.333333</td>
      <td>41.950000</td>
      <td>-114.310000</td>
      <td>5.000010</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For demonstrational purposes, we split the data set once randomly into training (70% of the data) and test data. We further standardize the feature variables as this allows us to interpret the importance of each variable by the absolute value of the estimated beta coefficient. The metric we use for assessing the predictive performance is the coefficient of determination.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cf_df</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">df_test</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">MedHouseVal</span><span class="p">,</span> <span class="n">df_test</span><span class="o">.</span><span class="n">MedHouseVal</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_s</span><span class="p">,</span> <span class="n">X_test_s</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R2 score for training data: </span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_s</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R2 score for training data: </span><span class="si">{</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_s</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cf_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;MedHouseVal&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_s</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cf_df</span><span class="o">.</span><span class="n">MedHouseVal</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_s</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R2 score for training data: 0.6093
R2 score for training data: 0.5958
</pre></div>
</div>
</div>
</div>
<p>The results regarding the <span class="math notranslate nohighlight">\(R^2\)</span> measure seem to be quite good. The estimated parameters below identify the median income (MedInc) and the location (Latidude and Longitude) of a house to be most important for the prediction of the median house value. This seems reasonable as location of real estates often drive their prices and people with higher income tend to live in area with more valuable houses.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">feature_name</span><span class="p">,</span> <span class="n">feature_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df_train</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Effect size of feature </span><span class="si">{</span><span class="n">feature_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">feature_value</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Effect size of feature MedInc: 0.85
Effect size of feature HouseAge: 0.12
Effect size of feature AveRooms: -0.30
Effect size of feature AveBedrms: 0.35
Effect size of feature Population: -0.00
Effect size of feature AveOccup: -0.04
Effect size of feature Latitude: -0.89
Effect size of feature Longitude: -0.87
</pre></div>
</div>
</div>
</div>
<p>When evaluating the errors made by the model, we detect a systematic underestimation of more valuable houses. Furthermore, true median house values seem to be truncated at the value 5 which may cause a bias in the estimation of parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;prediction error&quot;</span><span class="p">)</span>
<span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_vals</span><span class="p">,</span> <span class="n">m</span><span class="o">*</span><span class="n">y_vals</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;error trend&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Median house value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prediction error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d8b0aee8fa0400a19af9382b50dbabd03e73fd0b8dbb440e01bf458a53d4d3f4.png" src="_images/d8b0aee8fa0400a19af9382b50dbabd03e73fd0b8dbb440e01bf458a53d4d3f4.png" />
</div>
</div>
</section>
<section id="binary-classification">
<h3>Binary classification<a class="headerlink" href="#binary-classification" title="Link to this heading">#</a></h3>
<p>If the target variable is a categorical variable, the linear regression model does not provide reasonable predictions as all real-valued numbers are returned by the linear regression model, while the categorical variable only takes up to <span class="math notranslate nohighlight">\(K\)</span> values. Let us start with the scenario where the target variable has two possible outcomes, e.g., “positive” or “negative”. The first step is the numerical conversion of categories which is usually done by a dummy encoding. The choice which category is represented by <span class="math notranslate nohighlight">\(1\)</span> is arbitrary.</p>
<p><strong>Prediction</strong></p>
<p>The regression line serves as a building block for a model which predicts values for <span class="math notranslate nohighlight">\(y \in \lbrace 0, 1 \rbrace\)</span>. Let us denote the value of the regression line by <span class="math notranslate nohighlight">\(z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
z = \boldsymbol{\theta}^T \boldsymbol{x}
\]</div>
<p>One way to create <span class="math notranslate nohighlight">\(0, 1\)</span> predictions could be by transforming <span class="math notranslate nohighlight">\(z\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{y} = 
\begin{cases}
1 &amp; \text{for } z &gt; 0 \\
0 &amp; \text{else }
\end{cases}
\end{split}\]</div>
<p>However, a more popular choice is to transform <span class="math notranslate nohighlight">\(y^*\)</span> to a number <span class="math notranslate nohighlight">\(\pi \in [0, 1]\)</span> which is interpreted as the probability for <span class="math notranslate nohighlight">\(y\)</span> being of category <span class="math notranslate nohighlight">\(1\)</span>, i.e., <span class="math notranslate nohighlight">\(\pi = P\left(y = 1\right)\)</span>. While different functions can be chosen to transform <span class="math notranslate nohighlight">\(y^*\)</span> to <span class="math notranslate nohighlight">\(\pi\)</span>, a common one is the logistic function:</p>
<div class="math notranslate nohighlight">
\[
\pi = \frac{1}{1 + e^{-z}}
\]</div>
<p>To be more precise, we could also use the following notation:</p>
<div class="math notranslate nohighlight">
\[
\pi_{\boldsymbol{\theta}}\left( \boldsymbol{x} \right) = \frac{1}{1 + e^{-\boldsymbol{\theta}^T \boldsymbol{x}}}
\]</div>
<p>which highlights, that given information from feature variables <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, the probability prediction depends on the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. The final prediction needs to be build around a rule which probability threshold is used to assign predictions to the actual categories <span class="math notranslate nohighlight">\(0, 1\)</span>. A natural choice might be given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{y} = 
\begin{cases}
1 &amp; \text{for } \pi &gt; 0.5 \\
0 &amp; \text{else }
\end{cases}
\end{split}\]</div>
<p>However, the threshold can be set to other values in <span class="math notranslate nohighlight">\([0, 1]\)</span> which may improve the predictive performance.</p>
<p><strong>Model estimation</strong></p>
<p>Binary prediction models often use the loss function</p>
<div class="math notranslate nohighlight">
\[
L\left(\boldsymbol{\theta}\right) = \sum_{i=1}^m - y_i \log\left( \pi_i \right) - (1 - y_i)\log\left( 1- \pi_i \right)
\]</div>
<p>for estimating the model parameters. A closer look shows us that the loss function uses the negative value for the logarithm of the probability prediction for the observed category per observation. The plot below shows as that this value is lower if the probability prediction is high. This means a model is calibrated such that high probability predictions are generated for the category which is observed. This loss function is derived by the concept of entropy or by maximum-likelihood estimation, respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\pi$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\log \pi$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d394d47de333b7d399cbfdb3363fd7de5bf24daaf4181d43fec108ac037c6fe2.png" src="_images/d394d47de333b7d399cbfdb3363fd7de5bf24daaf4181d43fec108ac037c6fe2.png" />
</div>
</div>
<p><strong>Model evaluation</strong></p>
<p>Once, the model is trained, a first impression regarding the performance for a binary prediction model is given by the accuracy. Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
acc = 
\begin{cases}
1 &amp; \text{ if } y = \hat{y} \\
0 &amp; \text{ else }
\end{cases}
\end{split}\]</div>
<p>be a binary variable which is equal to <span class="math notranslate nohighlight">\(1\)</span> if the predicted category is in line with the observed category. The average for this variable represents the accuracy:</p>
<div class="math notranslate nohighlight">
\[
AC = \frac{1}{m} \sum_{i = 1}^n acc_i
\]</div>
<p>However, the accuracy does not distinguish between different mistakes the model can make. This is why we introduce the notion of positives and negatives. Independent of the context, positive refers to category <span class="math notranslate nohighlight">\(1\)</span> and negative to category <span class="math notranslate nohighlight">\(0\)</span>. The following scenarios can occur:</p>
<ul class="simple">
<li><p>The model predicts category <span class="math notranslate nohighlight">\(1\)</span> and the observation is of category <span class="math notranslate nohighlight">\(1\)</span>, this is called a true positive (TP)</p></li>
<li><p>The model predicts category <span class="math notranslate nohighlight">\(1\)</span> and the observation is of category <span class="math notranslate nohighlight">\(0\)</span>, this is called a false positive (FP)</p></li>
<li><p>The model predicts category <span class="math notranslate nohighlight">\(0\)</span> and the observation is of category <span class="math notranslate nohighlight">\(0\)</span>, this is called a true negative (TN)</p></li>
<li><p>The model predicts category <span class="math notranslate nohighlight">\(0\)</span> and the observation is of category <span class="math notranslate nohighlight">\(1\)</span>, this is called a false negative (FN)</p></li>
</ul>
<p>A visual inspection for the occurrence of these instances can be done by a confusion matrix as shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Example true labels and predicted labels</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Compute confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Display confusion matrix</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span> <span class="o">=</span> <span class="s2">&quot;viridis&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/582e2753260e4368b682e81d96e8093934347154ae05546b326a716be8ff2865.png" src="_images/582e2753260e4368b682e81d96e8093934347154ae05546b326a716be8ff2865.png" />
</div>
</div>
<p>With these definitions, the accuracy can also be written by:</p>
<div class="math notranslate nohighlight">
\[
AC = \frac{TP + TN}{TP + FN + TN + FN}
\]</div>
<p>To better differentiate between errors, the precision and recall of the model is often included. The precision is:</p>
<div class="math notranslate nohighlight">
\[
\text{precision} = \frac{TP}{TP + FP}
\]</div>
<p>where this metric indicates the ability of the model to generate as few false positive predictions as possible. The recall is defined by:</p>
<div class="math notranslate nohighlight">
\[
\text{recall} = \frac{TP}{TP + FN}
\]</div>
<p>it describes the ability of the model to identify the positive categories of the dependent variable. Since both measures are important sources of information on the quality of the model, they can be combined in the F1 score:</p>
<div class="math notranslate nohighlight">
\[
F_1 = \frac{2 \cdot \text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
\]</div>
<p>Similar to the linear regression model, one should always have in mind to evaluate these metrics in comparison to a simple yet reasonable benchmark. A reasonable benchmark for classification tasks is to always predict the category with the highest occurrence in the training data set. For instance, if we have a loan portfolio for which only 3$ firms default, the simple prediction would be to predict non-default for every company. This predictor would create an accuracy of 97%, however, would create many FN and no TP instances.</p>
</section>
<section id="multi-classification">
<h3>Multi-classification<a class="headerlink" href="#multi-classification" title="Link to this heading">#</a></h3>
<p>Once the idea of logistic regression is understood, it is relatively easy to adapt this idea to the case of a dependent variable with more than two categories. Two modeling approaches are popular for this purpose, the multinomial regression model and the softmax regression.</p>
<p><strong>Prediction</strong></p>
<p>In the multinomial model, <span class="math notranslate nohighlight">\(K-1\)</span> parameter vectors <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_k\)</span> are used for <span class="math notranslate nohighlight">\(K\)</span> categories, and in the softmax regression <span class="math notranslate nohighlight">\(K\)</span> parameter vectors <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_k\)</span> are used. No matter which model is used, we understand by a parameter vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\theta}_k = 
\begin{pmatrix}
\theta_{k0} \\
\theta_{k1} \\
\vdots \\
\theta_{kn} \\
\end{pmatrix}
\end{split}\]</div>
<p>if <span class="math notranslate nohighlight">\(n\)</span> independent variables are included in the model. In the multinomial model, we first have a linear regression alike function for the first <span class="math notranslate nohighlight">\(K-1\)</span> categories:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\theta_{10} + \theta_{11} x_1 + \theta_{12} x_2 + ... + \theta_{1n} x_n =  \boldsymbol{\theta}_1^T \boldsymbol{x} \\
\theta_{20} + \theta_{21} x_1 + \theta_{22} x_2 + ... + \theta_{2n} x_n =  \boldsymbol{\theta}_2^T \boldsymbol{x} \\
\vdots \\
\theta_{(K-1)0} + \theta_{(K-1)1} x_1 + \theta_{(K-1)2} x_2 + ... + \theta_{(K-1)n} x_n =  \boldsymbol{\theta}_{(K-1)}^T \boldsymbol{x} \\
\end{split}\]</div>
<p>As with the logistic regression model, we would have the problem for these values that the respective output is any real number, but we want to predict individual categories. Also for this type of modeling, instead of actual categories, probabilities <span class="math notranslate nohighlight">\(P\left(y = k | \boldsymbol{x}\right)\)</span> are predicted. However, translating the real numbers into probabilities requires a similar “trick” as using logistic regression. For a model with multiple categories, all probability predictions must hold that they are in the range of values <span class="math notranslate nohighlight">\([0, 1]\)</span>. In addition, they must sum to <span class="math notranslate nohighlight">\(1\)</span> because the categories are disjoint events. To achieve this, the real number values for each category in the multinomial model are transformed by:</p>
<div class="math notranslate nohighlight">
\[
P\left(y = k | \boldsymbol{x}\right) = f_{\boldsymbol{\theta}_k} \left( \boldsymbol{x} \right) = \frac{e^{ \boldsymbol{\theta}_k^T \boldsymbol{x}}}{1 + \sum_{l=1}^{K-1} e^{ \boldsymbol{\theta}_l^T \boldsymbol{x}}}
\]</div>
<p>The probability forecast of category <span class="math notranslate nohighlight">\(K\)</span> (which, by the way, can be chosen arbitrarily), results from the <span class="math notranslate nohighlight">\(K-1\)</span> probability forecasts:</p>
<div class="math notranslate nohighlight">
\[
P\left(y = K | \boldsymbol{x}\right) = 1 - \sum_{l=1}^{K-1} P\left(y = l | \boldsymbol{x}\right) = f_{\boldsymbol{\theta}_K} \left( \boldsymbol{x} \right) = \frac{1}{1 + \sum_{l=1}^{K-1} e^{ \boldsymbol{\theta}_l^T \boldsymbol{x}}}
\]</div>
<p>In softmax regression, regression lines are formed for all <span class="math notranslate nohighlight">\(K\)</span> categories:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\theta_{10} + \theta_{11} x_1 + \theta_{12} x_2 + ... + \theta_{1n} x_n =  \boldsymbol{\theta}_1^T \boldsymbol{x} \\
\theta_{20} + \theta_{21} x_1 + \theta_{22} x_2 + ... + \theta_{2n} x_n =  \boldsymbol{\theta}_2^T \boldsymbol{x} \\
\vdots \\
\theta_{K0} + \theta_{K1} x_1 + \theta_{K2} x_2 + ... + \theta_{Kp} x_p =  \boldsymbol{\theta}_{K}^T \boldsymbol{x} \\
\end{split}\]</div>
<p>Each real number of the respective straight lines is transformed into probabilities by means of the softmax function:</p>
<div class="math notranslate nohighlight">
\[
P\left(y = k | \boldsymbol{x}\right) = f_{\boldsymbol{\theta}_k} \left( \boldsymbol{x} \right) = \frac{e^{ \boldsymbol{\theta}_k^T \boldsymbol{x}}}{\sum_{l=1}^{K} e^{ \boldsymbol{\theta}_l^T \boldsymbol{x}}}
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(n+1\)</span> more parameters must be estimated for the softmax model, yet this approach is somewhat more popular in machine learning, whereas multinomial regression is increasingly used in statistical analysis. Even though these models use more parameters and more model equations than logistic regression, the way logistic regression is modeled remains similar. The starting point is always a real number which stems from linear regression function, which is transformed using an appropriate function depending on the modeling needs.</p>
<p><strong>Model estimation</strong></p>
<p>Also the models for <span class="math notranslate nohighlight">\(K&gt;2\)</span> categories, can be estimated under the same loss function. Let <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{y}}\)</span> be a <span class="math notranslate nohighlight">\(K\)</span> dimensional one-hot vector that has value <span class="math notranslate nohighlight">\(1\)</span> at position <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(0\)</span> at all other positions. Moreover, let <span class="math notranslate nohighlight">\( \boldsymbol{\pi}_{\Theta} \)</span> be the vector of probability forecasts, then all parameters <span class="math notranslate nohighlight">\(\Theta\)</span> of the model are obtained by minimizing the loss function:</p>
<div class="math notranslate nohighlight">
\[
L\left(\Theta\right) = -\frac{1}{m} \sum_{i=1}^m \tilde{\boldsymbol{y}}_i \log \left( \boldsymbol{\pi}_{\Theta, i} \right)
\]</div>
<p>Both model can be used for nominal categories. If the target variable is on an ordinal scale, the ranking of the variable must also be taken into account. This is done, for example, in the ordinal regression model.</p>
<p><strong>Model evaluation</strong></p>
<p>Multi-classification can be assessed in analogous use of the metrics for binary classification. See an example below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># Example true labels and predicted labels for multi-class classification</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Example probabilities for each class</span>

<span class="c1"># Accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Confusion Matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Classification Report</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Display results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix:&quot;</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span> <span class="o">=</span> <span class="s2">&quot;viridis&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification Report:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.75
Confusion Matrix:
</pre></div>
</div>
<img alt="_images/5b3368fef3498a387cfb168325087919fb31919ed41dceba1e1df0dc84be36fd.png" src="_images/5b3368fef3498a387cfb168325087919fb31919ed41dceba1e1df0dc84be36fd.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classification Report:
              precision    recall  f1-score   support

           0       0.86      0.86      0.86         7
           1       0.57      0.67      0.62         6
           2       0.83      0.71      0.77         7

    accuracy                           0.75        20
   macro avg       0.75      0.75      0.75        20
weighted avg       0.76      0.75      0.75        20
</pre></div>
</div>
</div>
</div>
<p><strong>Binary classification example</strong></p>
<p>The data set below demonstrates how to approach a binary classification problem. The data set contains information from bank customers. The target variable (Attrited customer) indicates if a customer stops to use the services of the bank. The goal is to identify customers who may leave the bank, given the feature variables. Feature variables contain categorical as well as numerical variables. We are going to one-hot-encode all categorical variables and standardized all feature variables. The cell below illustrates some general information. Furthermore, the bar plot for the target variable highlights the existence of an unbalanced target variable as the frequency of attrited customers is significantly lower than the one of existing customers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/BankChurners.csv&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;CLIENTNUM&#39;</span><span class="p">,</span>
                  <span class="s1">&#39;Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1&#39;</span><span class="p">,</span>
                  <span class="s1">&#39;Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 10127 entries, 0 to 10126
Data columns (total 20 columns):
 #   Column                    Non-Null Count  Dtype  
---  ------                    --------------  -----  
 0   Attrition_Flag            10127 non-null  object 
 1   Customer_Age              10127 non-null  int64  
 2   Gender                    10127 non-null  object 
 3   Dependent_count           10127 non-null  int64  
 4   Education_Level           10127 non-null  object 
 5   Marital_Status            10127 non-null  object 
 6   Income_Category           10127 non-null  object 
 7   Card_Category             10127 non-null  object 
 8   Months_on_book            10127 non-null  int64  
 9   Total_Relationship_Count  10127 non-null  int64  
 10  Months_Inactive_12_mon    10127 non-null  int64  
 11  Contacts_Count_12_mon     10127 non-null  int64  
 12  Credit_Limit              10127 non-null  float64
 13  Total_Revolving_Bal       10127 non-null  int64  
 14  Avg_Open_To_Buy           10127 non-null  float64
 15  Total_Amt_Chng_Q4_Q1      10127 non-null  float64
 16  Total_Trans_Amt           10127 non-null  int64  
 17  Total_Trans_Ct            10127 non-null  int64  
 18  Total_Ct_Chng_Q4_Q1       10127 non-null  float64
 19  Avg_Utilization_Ratio     10127 non-null  float64
dtypes: float64(5), int64(9), object(6)
memory usage: 1.5+ MB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># make target variable</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Attrition_Flag</span> <span class="o">==</span> <span class="s2">&quot;Attrited Customer&quot;</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;Attrition_Flag&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">categorical_variables</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="s1">&#39;object&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">numerical_variables</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">!=</span> <span class="s1">&#39;object&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">numerical_variables</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;target&quot;</span><span class="p">)</span>

<span class="n">labels</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">return_counts</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">freqs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Target label&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6c03d7a407c3f45ffabfb6c98850d5f053e9c5832b8a3e08317d11a881406186.png" src="_images/6c03d7a407c3f45ffabfb6c98850d5f053e9c5832b8a3e08317d11a881406186.png" />
</div>
</div>
<p>After estimation of the logistic regression model, we can take a look at the estimated parameters. Their absolute size give an indication which feature variables impact the probability for a leaving customer the most. For instance, the variable with the highest absolute value suggest to monitor customers with a low amount of transactions as a high value reduces the probability for the customer to leave.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">train_size</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;target&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">df_test</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;target&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">df_test</span><span class="o">.</span><span class="n">target</span>

<span class="n">numeric_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())])</span>
<span class="n">categorical_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s2">&quot;one_hot_encoder&quot;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">())])</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">transformers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;num&quot;</span><span class="p">,</span> <span class="n">numeric_transformer</span><span class="p">,</span> <span class="n">numerical_variables</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="n">categorical_transformer</span><span class="p">,</span> <span class="n">categorical_variables</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">preprocessor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">X_train_s</span><span class="p">,</span> <span class="n">X_test_s</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">logistic_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">logistic_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_s</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">preprocessor</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="k">for</span> <span class="n">feature_name</span><span class="p">,</span> <span class="n">feature_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">logistic_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Effect size of feature </span><span class="si">{</span><span class="n">feature_name</span><span class="si">}</span><span class="s2">:  </span><span class="si">{</span><span class="n">feature_value</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Effect size of feature num__Customer_Age:  0.05
Effect size of feature num__Dependent_count:  0.14
Effect size of feature num__Months_on_book:  -0.10
Effect size of feature num__Total_Relationship_Count:  -0.69
Effect size of feature num__Months_Inactive_12_mon:  0.52
Effect size of feature num__Contacts_Count_12_mon:  0.58
Effect size of feature num__Credit_Limit:  -0.11
Effect size of feature num__Total_Revolving_Bal:  -0.71
Effect size of feature num__Avg_Open_To_Buy:  -0.05
Effect size of feature num__Total_Amt_Chng_Q4_Q1:  -0.11
Effect size of feature num__Total_Trans_Amt:  1.57
Effect size of feature num__Total_Trans_Ct:  -2.72
Effect size of feature num__Total_Ct_Chng_Q4_Q1:  -0.62
Effect size of feature num__Avg_Utilization_Ratio:  -0.11
Effect size of feature cat__Gender_F:  0.03
Effect size of feature cat__Gender_M:  -0.92
Effect size of feature cat__Education_Level_College:  -0.39
Effect size of feature cat__Education_Level_Doctorate:  0.19
Effect size of feature cat__Education_Level_Graduate:  -0.22
Effect size of feature cat__Education_Level_High School:  -0.27
Effect size of feature cat__Education_Level_Post-Graduate:  -0.01
Effect size of feature cat__Education_Level_Uneducated:  -0.15
Effect size of feature cat__Education_Level_Unknown:  -0.04
Effect size of feature cat__Marital_Status_Divorced:  -0.04
Effect size of feature cat__Marital_Status_Married:  -0.59
Effect size of feature cat__Marital_Status_Single:  -0.03
Effect size of feature cat__Marital_Status_Unknown:  -0.24
Effect size of feature cat__Income_Category_$120K +:  0.35
Effect size of feature cat__Income_Category_$40K - $60K:  -0.53
Effect size of feature cat__Income_Category_$60K - $80K:  -0.08
Effect size of feature cat__Income_Category_$80K - $120K:  0.11
Effect size of feature cat__Income_Category_Less than $40K:  -0.37
Effect size of feature cat__Income_Category_Unknown:  -0.36
Effect size of feature cat__Card_Category_Blue:  -0.73
Effect size of feature cat__Card_Category_Gold:  0.35
Effect size of feature cat__Card_Category_Platinum:  -0.03
Effect size of feature cat__Card_Category_Silver:  -0.47
</pre></div>
</div>
</div>
</div>
<p>To better understand where the model makes mistakes and how its performance is overall, we can use the confusion matrix together with proper metrics such as precision and recall. The confusion matrix illustrates asymmetry w.r.t. to errors made. False negatives are conducted more often than false positives. As a result, recall scores are lower compared to precision scores. Overall, this indicates that our model may not be good in detection enough actual customers who leave the company.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">y_train_hat</span><span class="p">,</span> <span class="n">y_test_hat</span> <span class="o">=</span> <span class="n">logistic_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_s</span><span class="p">),</span> <span class="n">logistic_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_s</span><span class="p">)</span>
<span class="n">cm_train</span><span class="p">,</span> <span class="n">cm_test</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_hat</span><span class="p">),</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_hat</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm_train</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predictions&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True Labels&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm_test</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Predictions&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;True Labels&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Test data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ddb65b41b46e48848f9be3f827e3fc16046d42199cfc6f903ecbde127b8f4958.png" src="_images/ddb65b41b46e48848f9be3f827e3fc16046d42199cfc6f903ecbde127b8f4958.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">f1_score</span>

<span class="n">recall_train</span><span class="p">,</span> <span class="n">recall_test</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_hat</span><span class="p">),</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_hat</span><span class="p">)</span>
<span class="n">precision_train</span><span class="p">,</span> <span class="n">precision_test</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_hat</span><span class="p">),</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_hat</span><span class="p">)</span>
<span class="n">f1_score_train</span><span class="p">,</span> <span class="n">f1_score_test</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_hat</span><span class="p">),</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_hat</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;recall&quot;</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">],</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">])</span>
<span class="n">scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;training&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">recall_train</span><span class="p">,</span> <span class="n">precision_train</span><span class="p">,</span> <span class="n">f1_score_train</span><span class="p">]</span>
<span class="n">scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;test&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">recall_test</span><span class="p">,</span> <span class="n">precision_test</span><span class="p">,</span> <span class="n">f1_score_test</span><span class="p">]</span>
<span class="n">scores</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>training</th>
      <th>test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>recall</th>
      <td>0.589744</td>
      <td>0.546371</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.772885</td>
      <td>0.767705</td>
    </tr>
    <tr>
      <th>f1</th>
      <td>0.669007</td>
      <td>0.638398</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="neural-networks">
<h3>Neural networks<a class="headerlink" href="#neural-networks" title="Link to this heading">#</a></h3>
<p>The models presented above are limited in terms of capture complex and flexible relationships between feature and target variables. While a multitude of more flexible models such as decision and regression trees, random forests, gradient boosted models, support vector machines, etc. exists, we restrict ourselves to one powerful model class - <em>neural networks</em>.</p>
<p>Neural networks can be used for all types of supervised tasks and are able to capture complex relationships. Furthermore, they also are used for unsupervised and reinforcement learning as well which demonstrates their importance and versatile for the machine learning domain. While different architectures of neural networks, e.g., recurrent networks, encoder-decoder network, generative networks, exist, we take a look one type of neural network architecture - forward neural networks (other names for this architecture are multilayer perceptron, fully connected neural network, densely connected neural networks).</p>
<p>Before we take a look at the formal definition for neural networks, let us subsume the most important characteristics of neural networks:</p>
<ul class="simple">
<li><p>Neural networks consist of <em>layers</em>, an input layer, hidden layers and an output layer</p></li>
<li><p>Each layer includes <em>neurons</em>; the number of neurons in the hidden layers can be set by the user as desired</p></li>
<li><p>Neurons (with the exception of input neurons) are generated by the composition of an <em>affine transformation</em> <span class="math notranslate nohighlight">\(f(x) = a + b x\)</span> and an arbitrary <em>activation function</em> <span class="math notranslate nohighlight">\(g\)</span>, each neuron is made of the form: <span class="math notranslate nohighlight">\(g\left(f(x)\right)\)</span></p></li>
<li><p>Parameters of the network can be learned by using the same loss functions as for the models above</p></li>
<li><p>Parameter estimation is done by numerical optimization of the loss function using algorithms which are based upon <em>gradient descent</em></p></li>
</ul>
<p>With this in mind, let us take a look at the formal definition. As it is very common, we use <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> as the symbols for the parameters of neural networks, these are called <em>weights</em> and <em>bias</em>.</p>
<p>The first layer which receives input data is called the input layer, the last layer in a neural network returns processed input and is called the output layer. In between those two layers hidden layers can be used. Each layer consists of a number of neurons which is defined by the user. Furthermore, the output of each layer is activated by activation functions.</p>
<p>First, let us take a look how a neuron is determined using neurons from the previous layer for a single observation in the data set. The input data is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{x} = 
\begin{pmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_{n_1} \\
\end{pmatrix}
\end{split}\]</div>
<p>where the index <span class="math notranslate nohighlight">\(j = 1, ..., n_1\)</span> represents the number of input variables (input neurons) in the first layer <span class="math notranslate nohighlight">\(n_{l=1}\)</span>. For the sake of consistency, let us denote the input variables by <span class="math notranslate nohighlight">\(\boldsymbol{x} = \boldsymbol{h}^{(1)}\)</span>, where the <span class="math notranslate nohighlight">\((1)\)</span> in the superscript signals that these are the neurons from the first layer. The first step to determine the neuron in the next layer ist to aggregate the neurons of the previous layer in a weighted fashion. This is called affine map:</p>
<div class="math notranslate nohighlight">
\[
z^{(2)} = \boldsymbol{h}^{(1), T} \boldsymbol{w}^{(1)}  + b^{(1)} = w_{1}^{(1)} h_{1}^{(1)} + w_{2}^{(1)} h_2^{(1)} + ... + w_{n_{1}}^{(1)} h_{n_{1}}^{(1)} + b^{(1)}
\]</div>
<p>The output of this transformation is used as input for an activation function <span class="math notranslate nohighlight">\(g\)</span>. Various activation functions are used for neural networks. Popular examples are</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Identity</p></td>
<td><p><span class="math notranslate nohighlight">\(g(z) = z\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Relu</p></td>
<td><p><span class="math notranslate nohighlight">\(g(z) = \max (z, 0)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Sigmoid</p></td>
<td><p><span class="math notranslate nohighlight">\(g(z) = \frac{1}{1 + e^{-z}} \)</span></p></td>
</tr>
</tbody>
</table>
<p>See the output below for their visualization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Identity&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Relu&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Sigmoid&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cba40e5b10272ac85c924d86df804c1032673b6503091c35a153c420ec43817d.png" src="_images/cba40e5b10272ac85c924d86df804c1032673b6503091c35a153c420ec43817d.png" />
</div>
</div>
<p>The result of the activation function is the hidden neuron for the next layer. Thus, to create a single neuron for the next layer, we use:</p>
<div class="math notranslate nohighlight">
\[
h^{(2)} = g(z^{(2)}) = g \left(w_{1}^{(1)} h_{1}^{(1)} + w_{2}^{(1)} h_2^{(1)} + ... + w_{n_{1}}^{(1)} h_{n_{1}}^{(1)} + b^{(1)}\right)
\]</div>
<p>If we want to generate more than one hidden neuron, we need different parameters. Let us introduce the index <span class="math notranslate nohighlight">\(k\)</span> for this purpose:</p>
<div class="math notranslate nohighlight">
\[
h_k^{(2)} = g(z_k^{(2)}) = g \left(w_{1k}^{(1)} h_{1}^{(1)} + w_{2k}^{(1)} h_2^{(1)} + ... + w_{n_{1}k}^{(1)} h_{n_{1}}^{(1)} + b_k^{(1)}\right)
\]</div>
<p>As this operation is used in general from layer <span class="math notranslate nohighlight">\(l-1\)</span> to layer <span class="math notranslate nohighlight">\(l\)</span>, we can write:</p>
<div class="math notranslate nohighlight">
\[
h_k^{(l)} = g(z_k^{(l)}) = g \left(w_{1k}^{(l-1)} h_{1}^{(l-1)} + w_{2k}^{(l-1)} h_2^{(l-1)} + ... + w_{n_{l-1}k}^{(l-1)} h_{n_{l-1}}^{(l-1)} + b_k^{(l-1)}\right) = g \left( \boldsymbol{h}^{(l-1), T} \boldsymbol{w}_k^{(l-1)} + b_k^{(l-1)} \right) 
\]</div>
<p>Given a data set with <span class="math notranslate nohighlight">\(m\)</span> observations, we can use the expression:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H}^{(l)} = g \left( \boldsymbol{H}^{(l-1)} \boldsymbol{W}^{(l-1)}  + \boldsymbol{b}^{(l-1)} \right)
\]</div>
<p>Here, the dimensions are as follow:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{H}^{(l-1)} \in \mathbb{R}^{m \times n_{l-1}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{H}^{(l)} \in \mathbb{R}^{m \times n_{l}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{W}^{(l-1)} \in \mathbb{R}^{n_{l-1} \times n_{l}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{b}^{(l-1)} \in \mathbb{R}^{n_{l}}\)</span></p></li>
</ul>
<p>and the operation <span class="math notranslate nohighlight">\(\boldsymbol{H}^{(l-1)} \boldsymbol{W}^{(l-1)} + \boldsymbol{b}^{(l-1)}\)</span> is defined in a way such that the vector <span class="math notranslate nohighlight">\(\boldsymbol{b}^{(l-1)}\)</span> is added to each column of the matrix <span class="math notranslate nohighlight">\(\boldsymbol{H}^{(l-1)} \boldsymbol{W}^{(l-1)}\)</span> (this is called broadcasting).</p>
<p>These operations are repeated for as many layers as desired by the user. For instance, let us take a look how data would be processed for a neural network with one hidden layer, two hidden neurons and one neuron in the output layer. Let us further focus on the processing of a single observation and assume that the network receives realizations of three input variables <span class="math notranslate nohighlight">\(x_1, x_2, x_3\)</span> in the input layer and only the identity function is used for activation in each layer.</p>
<p>The input neurons are <span class="math notranslate nohighlight">\(h_1^{(1)} = x_1, h_2^{(1)} = x_2, h_3^{(1)} = x_3\)</span>. The hidden neurons for the next layer are calculated by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\boldsymbol{h}^{(2), T} = g \left(
\begin{pmatrix}
h_1^{(1)} &amp; h_2^{(1)} &amp; h_3^{(1)} \\
\end{pmatrix}
\begin{pmatrix}
w_{11}^{(1)} &amp; w_{12}^{(1)}  \\
w_{21}^{(1)} &amp; w_{22}^{(1)}  \\
w_{31}^{(1)} &amp; w_{32}^{(1)}  \\
\end{pmatrix}  +
\begin{pmatrix}
b_{1}^{(1)}  \\
b_{2}^{(1)}  \\
\end{pmatrix} 
\right) = \\ =
\begin{pmatrix}
h_1^{(1)} w_{11}^{(1)} + h_2^{(1)} w_{21}^{(1)} + h_3^{(1)} w_{31}^{(1)}  &amp; 
h_1^{(1)} w_{12}^{(1)} + h_2^{(1)} w_{22}^{(1)} + h_3^{(1)} w_{32}^{(1)} \\
\end{pmatrix} +
\begin{pmatrix}
b_{1}^{(1)}  \\
b_{2}^{(1)}  \\
\end{pmatrix} =  \\ =
\begin{pmatrix}
h_1^{(1)} w_{11}^{(1)} + h_2^{(1)} w_{21}^{(1)} + h_3^{(1)} w_{31}^{(1)} + b_{1}^{(1)}   &amp; 
h_1^{(1)} w_{12}^{(1)} + h_2^{(1)} w_{22}^{(1)} + h_3^{(1)} w_{32}^{(1)} + b_{2}^{(1)} \\
\end{pmatrix} +
\end{align}
\end{split}\]</div>
<p>To produce the output of the neural network, we further process these neurons through the second (output) layer by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
h^{(3)} = g \left(
\begin{pmatrix}
h_1^{(2)} &amp; h_2^{(2)} \\
\end{pmatrix} 
\begin{pmatrix}
w_{1}^{(2)} \\ w_{2}^{(2)}  \\
\end{pmatrix} +
b^{(2)}
\right) = 
w_{1}^{(2)} h_1^{(2)} + w_{2}^{(2)} h_2^{(2)} + b^{(2)}
\end{split}\]</div>
<p>The only thing which is fixed here are the input values <span class="math notranslate nohighlight">\(h_1^{(1)} = x_1, h_2^{(1)} = x_2, h_3^{(1)} = x_3\)</span>. All other values <span class="math notranslate nohighlight">\(\boldsymbol{W}^{(1)}, \boldsymbol{b}^{(1)}, \boldsymbol{W}^{(2)}, \boldsymbol{b}^{(2)}\)</span> are parameters which need to be trained for the model. The number of parameters is determined by the number of hidden neurons which we set for every layer. The more hidden neurons we would like to use, the higher the number of parameters which needs to be trained. What we see in the example above is what is called a forward pass which describes how the input data is processed through the network. Note that there are no real limits regarding the architecture. For instance you can also define a neural network which produces more than one neuron in the output layer. This is why a neural network can be used for regression and classification tasks as well. The only consideration which need to be taken care of are:</p>
<ul class="simple">
<li><p>For a regression task, the output neuron must include a single neuron which is activated by the identity function</p></li>
<li><p>For a binary classification task, the output neuron must include a single neuron which is activated by the logistic function (which is also called sigmoid function in this context)</p></li>
<li><p>For a multi-classification task, the number of output neurons is equal to the number of categories; the neurons are activated by the softmax function</p></li>
</ul>
<p>In order to train the parameters we minimize a loss function which is appropriate for the task. For instance, the loss functions from above can be used to train a neural network for a regression or classification task, respectively. The optimization is done by iterative and numerical methods which rely upon gradient information. The general iterative procedure for <em>gradient descent</em> algorithms is:</p>
<ol class="arabic simple">
<li><p>Start with a random guess for the set of all parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span></p></li>
<li><p>Until a condition for termination is fulfilled, repeat:</p>
<ol class="arabic simple">
<li><p>Determine <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{\theta}} L\)</span></p></li>
<li><p>Update the parameter by: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(k)} \leftarrow \boldsymbol{\theta}^{(k-1)} - \eta \nabla_{\boldsymbol{\theta}} L\)</span></p></li>
</ol>
</li>
</ol>
<p>The gradient <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{\theta}} L\)</span> includes all partial derivatives if we derive the loss function w.r.t. to a single parameter of the model. If this value is positive, it tells us that increasing the parameter by a small amount would increase the loss which is why we rather should decrease the parameter. If this value is negative, we should increase the parameter as this reduces the loss. This rule is subsumed by 2.2. The gradient is calculated by automatic differentiation which can be done by application of the chain rule. It dictates you how to derive derivatives for compositions of functions. Given the function <span class="math notranslate nohighlight">\(h: g \circ f\)</span> with <span class="math notranslate nohighlight">\(h(x) = g\left( f \left(x \right) \right)\)</span>, the derivative is determined by:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial h}{\partial x} = \frac{\partial g}{\partial f} \frac{\partial f}{\partial x}
\]</div>
<p>The forward neural network is a composition of mathematical operations:</p>
<div class="math notranslate nohighlight">
\[
g^{(L)} \left(f^{(L)} \left( ... \left( g^{(1)} \left( f^{(1)} \left( \boldsymbol{x} \right) \right) \right) \right) \right)
\]</div>
<p>This is why each partial derivative can be determined applying the chain rule. As the outer functions in the formula above represent the last operations of the network, we need these derivatives first. This is why we <em>backpropagate</em> through the network from output to input when deriving all partial derivatives.</p>
<p>Certain variations and improvements of pure gradient descent are used for training neural networks, however, a basic understanding how gradient information is provided by the description above. In more detail, a popular version is so called mini-batch gradient descent which splits a data set into subsamples (batches) and derives parameter updates for every batch. One run through all batches is called an epoch. Training is usually one over multiple epochs until the loss value converges to a minimum. Variables which impact the neural network’s performance, but are not trained by minimizing the loss function are called <em>hyperparameters</em>. For instance, the number of layers, number of hidden neurons, activation functions, learning rate, etc. can also further improve the performance.</p>
<p>Furthermore, the following aspects need to be considered when training neural networks to data:</p>
<ul class="simple">
<li><p>Numerical data should be normalized by, e.g., standardization or min-max normalization to avoid unwanted weighting of individual variables and numerical problems for gradient calculation</p></li>
<li><p>Neural networks are very flexible and can be overfitted; this should be taken care of by regularization techniques such as L1 or L2 regularization or a mechanism called <em>dropout</em></p></li>
</ul>
</section>
<section id="validation-and-generalization-of-supervised-learning-models">
<h3>Validation and generalization of supervised learning models<a class="headerlink" href="#validation-and-generalization-of-supervised-learning-models" title="Link to this heading">#</a></h3>
<p>Machine learning is based on learning from data samples. The overall goal is to develop models which learn general relationships that hold for new and unseen data. Thus, a simple best practice is to split data into <em>training</em> and <em>test</em> data, train the model for test data and evaluate its performance w.r.t. the test data. However, if we only follow this procedure, evaluation metrics for test data are exposed to a large level of variation. This means, depending how we split the data, the evaluation of test data may substantially differ. One way to deal with this is to conduct <em>cross-validation</em>. A popular choice for cross validation is K-fold cross validation. In K-fold cross validation, the data set is split into <span class="math notranslate nohighlight">\(k\)</span> equal parts. Then, one of the parts is used as a test data set at a time, while the remaining data is used to train the model. This process is performed <span class="math notranslate nohighlight">\(k\)</span> times by iterating over all the parts (folds). The graph below visualizes this form of data partitioning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">cmap_cv</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">400</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="p">(</span><span class="n">tr</span><span class="p">,</span> <span class="n">tt</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
    <span class="c1"># Fill in indices with the training/test groups</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">indices</span><span class="p">[</span><span class="n">tt</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">indices</span><span class="p">[</span><span class="n">tr</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Visualize the results</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
            <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)),</span>
            <span class="p">[</span><span class="n">ii</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span>
            <span class="n">c</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;_&quot;</span><span class="p">,</span>
            <span class="n">lw</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_cv</span><span class="p">,</span>
            <span class="n">vmin</span><span class="o">=-</span><span class="mf">0.2</span><span class="p">,</span>
            <span class="n">vmax</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>
        <span class="p">)</span>


<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">201</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Observation number&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Cross validation number&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;K-Fold&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ba2c0cf229f7b1b4ed6e237c08df349f715f492b7f5501dfa62041b089d2d6d8.png" src="_images/ba2c0cf229f7b1b4ed6e237c08df349f715f492b7f5501dfa62041b089d2d6d8.png" />
</div>
</div>
<p>However, this validation technique assumes that observations are conditionally independent. This means no dependencies such as spatial or temporal dependence is given which can not be explained by the feature variables. For financial time series data, this assumption is at least critical. Furthermore, as financial data often is sequential, splitting the data as for K-fold cross validation means we can use data from the future to predict the past.</p>
<p>To make sure that such potential mis-specifications do not impact the assessment of a model’s performance, one either may want to use a form of time series validation. Two choices are natural candidates with this respect: (1) a rolling window approach and (2) a cumulative time approach. The rolling window approach always uses the same number of time steps from the past <span class="math notranslate nohighlight">\(t-T^{\text{train}}, t-T^{\text{train}}+1, ..., t-1, t\)</span> to train a model and a fixed number of time steps from the future <span class="math notranslate nohighlight">\(t+1, t+2, ...., t + T^{\text{test}} - 1, t + T^{\text{test}}\)</span> is used for evaluation. If you want to make sure that training data does not include information from the test data, you can omit a certain number of time steps between training and test data. Furthermore, if possible the test data sets should not overlap. The cumulative approach works almost in the same way, however, instead of keeping the training size constant, one always adds more data to the training data set as time passes. The graphics below visualize both approaches for which no time steps are omitted during training and testing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">y_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">40</span><span class="p">):</span>
    <span class="n">tr</span> <span class="o">=</span> <span class="n">i</span><span class="o">-</span><span class="mi">40</span>
    <span class="n">tt</span> <span class="o">=</span> <span class="n">i</span><span class="o">+</span><span class="mi">40</span>
    <span class="n">full_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span>
    <span class="n">indices_roll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">200</span><span class="p">)</span> 
    <span class="n">indices_roll</span><span class="p">[</span><span class="n">tr</span><span class="p">:</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">indices_roll</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">tt</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">indices_cumulative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">200</span><span class="p">)</span> 
    <span class="n">indices_cumulative</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">indices_cumulative</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">tt</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Visualize the results</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_roll</span><span class="p">)),</span>
        <span class="p">[</span><span class="n">y_idx</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_roll</span><span class="p">),</span>
        <span class="n">c</span><span class="o">=</span><span class="n">indices_roll</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;_&quot;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_cv</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=-</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Visualize the results</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_cumulative</span><span class="p">)),</span>
        <span class="p">[</span><span class="n">y_idx</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_cumulative</span><span class="p">),</span>
        <span class="n">c</span><span class="o">=</span><span class="n">indices_cumulative</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;_&quot;</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_cv</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=-</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">y_idx</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">201</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)])</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Observation number&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Validation number&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Rolling window&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">201</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)])</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Observation number&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Validation number&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cumulative window&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02b34f46d9801e4c1440d21beb827d19918bc177e48f9bf57bbaec6ff551368e.png" src="_images/02b34f46d9801e4c1440d21beb827d19918bc177e48f9bf57bbaec6ff551368e.png" />
</div>
</div>
<p>If a model has hyperparameters, the validation process can be repeated for different sets of hyperparameters. Several algorithms exist for hyperparameter tuning. Examples are:</p>
<ul class="simple">
<li><p>Grid Search: Exhaustive search over a specified parameter grid.</p></li>
<li><p>Random Search: Randomly sample the parameter space.</p></li>
<li><p>Genetic Algorithms: Use evolutionary algorithms to optimize hyperparameters.</p></li>
</ul>
<p>Implementations can be found in packages such as Optuna, Hyperopt, or Scikit-learn’s GridSearchCV and RandomizedSearchCV.</p>
</section>
</section>
<section id="unsupervised-learning">
<h2>Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading">#</a></h2>
<p>While the field of unsupervised learning includes a great variety of algorithms with different goals, two very popular applications are dimensionality reduction and clustering. Dimensionality reduction may fulfill different purposes. If data includes a large number of variables, we may refer to it as high dimensional. A high number of dimensions exposes data analysis to different challenges. First, we only can visualize data up to three dimensions. Every data set with higher dimensions can not be visualized at once. Second, the curse of dimensionality challenges machine learning algorithms to reach their goal of learning general relationships in the data. In short, data becomes more sparse in higher dimensions which means that observations tend to be drawn apart from each other in the numerical space. As general relationships usually are found at dense areas of observations, it becomes harder to find them in higher dimensions. This is why dimensionality reduction techniques are often combined with other tasks such as supervised learning or clustering. For instance, a dimensionality reduction technique may be used to reduce the input dimension of data and use the reduced form for training a prediction model. The goal of clustering data is achieved if the dataset is split into sub-groups whose members are similar to each other, but, different to members of other sub-groups.</p>
<p>In analogy to the supervised learning chapter before, we aim to present rather simple and popular approaches for dimensionality reduction and clustering. W.r.t to dimensionality reduction, we take a look at <em>principal component analysis</em> (PCA) and to the <em>k-means</em> algorithm for presenting an example of a popular clustering algorithm.</p>
<section id="principal-component-analysis">
<h3>Principal component analysis<a class="headerlink" href="#principal-component-analysis" title="Link to this heading">#</a></h3>
<p>PCA is a statistical technique used to reduce the dimensionality of a dataset while retaining most of the variation in the data. This is achieved by transforming the original variables into a new set of uncorrelated variables called principal components. These components are ordered by the amount of variance they capture from the data.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a matrix with data including <span class="math notranslate nohighlight">\(m\)</span> observations of <span class="math notranslate nohighlight">\(n\)</span> variables:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = 
\begin{pmatrix}
x_{11} &amp; ... &amp; x_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{m1} &amp; ... &amp; x_{mn} \\
\end{pmatrix}
\end{split}\]</div>
<p>then, <span class="math notranslate nohighlight">\(X\)</span> must at least be mean-standardized or can be fully standardized by:</p>
<div class="math notranslate nohighlight">
\[
z_j = \frac{x_j - \mu_j}{\sigma_j}
\]</div>
<p>where <span class="math notranslate nohighlight">\(j\)</span> represents the index for variable <span class="math notranslate nohighlight">\(j\)</span>. Choosing full standardization or mean-standardization, only, changes the lower dimensional reduction. The former should be done, if one aims to put an emphasis on the dependencies. To create a lower dimensional version of the data set, one uses vectors <span class="math notranslate nohighlight">\(\boldsymbol{u}_l,~l=1,...,n\)</span>. For an actual reduction, we use a number of these vectors <span class="math notranslate nohighlight">\(k\)</span> which is lower than the original number of the variables, <span class="math notranslate nohighlight">\(l &lt; n\)</span>. Let <span class="math notranslate nohighlight">\(U_k\)</span> be the matrix with <span class="math notranslate nohighlight">\(l=1, ..., k\)</span> vectors, than, the reduced form of the data is:</p>
<div class="math notranslate nohighlight">
\[
V = ZU_k
\]</div>
<p>The vectors are called principal components while the columns of <span class="math notranslate nohighlight">\(V\)</span> are principal component scores. Principal components can be derived by two ways: (1) maximizing the variance of all projections <span class="math notranslate nohighlight">\(Z \boldsymbol{u}_l\)</span> subject to the constraint that all vectors <span class="math notranslate nohighlight">\(\boldsymbol{u}_l\)</span> are orthonormal. The latter means that each vector is a unit vector that is independent to the other vectors; (2), the vectors can be found by minimizing the reconstruction error. Reconstructions of the PCA projections are given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{Z} = ZU_kU_k^T = V U_k^T
\]</div>
<p>These values are predictions for the original data, given only the reduced form of the data is observable. The better, the original data can be regenerated by the lower dimensional reduction, the better it is. Technically, this means we minimize:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\min_{V_k} &amp; ||Z - \hat{Z}||_F^2 \\
\text{subject to:} &amp; \boldsymbol{u}_j \boldsymbol{u}_j^T = 1, \forall j=1,...,n \\
&amp; \boldsymbol{u}_i \boldsymbol{u}_j^T = 0, \forall i \neq j
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\( || \cdot ||_F^2\)</span> denotes the squared Frobenius norm (for this application it is the sum of all squared deviations of data and regenerated data).</p>
<p>The solution for finding the principal components is usually derived by eigenvalue or singular value decomposition of the covariance or correlation matrix of the data, respectively. Furthermore, principal components are ordered decreasingly, such that the first principal component explains the variation in the data the most. As data is at least mean-centered, the overall level of variation is given by:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m \sum_{j=1}^n z_{ij}^2
\]</div>
<p>while the variation of the projected data is:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m \sum_{l=1}^k v_{il}^2
\]</div>
<p>The explained variance ratio is simply:</p>
<div class="math notranslate nohighlight">
\[
\frac{\sum_{i=1}^m \sum_{l=1}^k v_{il}^2}{\sum_{i=1}^m \sum_{j=1}^n z_{ij}^2}
\]</div>
<p>By its construction, the transformation from the original data to its reduced dimensional form is done by linear. Examples for non-linear approaches are the t-distributed stochastic neighbor embedding (t-SNE) or the uniform manifold approximation and projection (UMAP) algorithms. While these are more flexible how lower dimensional representation of the data are generated, PCA is easier to interpret as we are going to see in examples later on.</p>
<p><strong>Example: PCA for asset returns</strong></p>
<p>The cell below first illustrates how PCA is conducted for mean-centered asset return data. Using mean-centered data results in principal components which are derived upon the covariance matrix of the data which is also exhibited. Afterwards, we examine results for standardized data which results in principal components that are derived upon the correlation matrix. Thus, for the first approach, individual variances impact the derivation, while, for the second approach, correlation is more in the focus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">sp_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/sp500_close_prices.csv&quot;</span><span class="p">)</span>
<span class="n">discrete_returns</span> <span class="o">=</span> <span class="n">sp_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;AAPL&quot;</span><span class="p">,</span> <span class="s2">&quot;MSFT&quot;</span><span class="p">,</span> <span class="s2">&quot;MS&quot;</span><span class="p">,</span> <span class="s2">&quot;JPM&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">discrete_returns</span> <span class="o">-</span> <span class="n">discrete_returns</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="n">V</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pca_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">))]</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">cumsum</span><span class="p">())</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cumulative explained variance ratio&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">V</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">V</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;PCA score 1&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;PCA score 2&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3fcc3ca959ded0946a87ed147c13e176baa133220294a4d727324f6c6e42a0b0.png" src="_images/3fcc3ca959ded0946a87ed147c13e176baa133220294a4d727324f6c6e42a0b0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pcs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">discrete_returns</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;PCA 1&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA 2&quot;</span><span class="p">])</span>
<span class="n">pcs</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>MSFT</th>
      <th>MS</th>
      <th>JPM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>PCA 1</th>
      <td>0.3354</td>
      <td>0.4662</td>
      <td>0.7345</td>
      <td>0.3614</td>
    </tr>
    <tr>
      <th>PCA 2</th>
      <td>0.6114</td>
      <td>0.5526</td>
      <td>-0.5232</td>
      <td>-0.2169</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For both approaches almost 80% of the variation can be explained by only two variables (scores of the first two principal components). Regarding the first approach, we observe that the signs of all values from the first principle component are identical (positive), while two companies each have either a positive or negative sign regarding the weighting of the second principal component.</p>
<p>This makes sense considering the background information that equity asset returns are usually positively correlated and that two companies are from the banking sector while the other two are from the technology sector. The first principal component captures the positive dependence among all companies. Ignoring the difference in weights of first principal component vector, its score is positive on days which on (weighted) average exhibit positive returns for all companies and negative values vice versa. Regarding the second component, its score being positive indicates better performance of Apple and Microsoft than for Morgan Stanley and JP Morgan.</p>
<p>Taking a look at the covariance matrix and the row-wise sum of it, we observe that Morgan Stanley seems to have the highest contribution of individual variance and pairwise variances. This is the reason for its highest weight in the first principal component.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">discrete_returns</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>MSFT</th>
      <th>MS</th>
      <th>JPM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AAPL</th>
      <td>0.000161</td>
      <td>0.000080</td>
      <td>0.000013</td>
      <td>0.000014</td>
    </tr>
    <tr>
      <th>MSFT</th>
      <td>0.000080</td>
      <td>0.000177</td>
      <td>0.000042</td>
      <td>0.000020</td>
    </tr>
    <tr>
      <th>MS</th>
      <td>0.000013</td>
      <td>0.000042</td>
      <td>0.000242</td>
      <td>0.000083</td>
    </tr>
    <tr>
      <th>JPM</th>
      <td>0.000014</td>
      <td>0.000020</td>
      <td>0.000083</td>
      <td>0.000108</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">discrete_returns</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AAPL    0.000268
MSFT    0.000318
MS      0.000380
JPM     0.000225
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>The results for the approach using the correlation matrix results in similar observations. The signs of the first principal component are equal. In comparison to the results above, the sign is negative for all companies, however, this just means that negative scores of the first principal component indicate on average good days for all companies. Furthermore, the weightings of principal components differ as pairwise dependencies are in the focus of their determination while individual variances play a less important role in comparison to before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">sp_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/sp500_close_prices.csv&quot;</span><span class="p">)</span>
<span class="n">discrete_returns</span> <span class="o">=</span> <span class="n">sp_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;AAPL&quot;</span><span class="p">,</span> <span class="s2">&quot;MSFT&quot;</span><span class="p">,</span> <span class="s2">&quot;MS&quot;</span><span class="p">,</span> <span class="s2">&quot;JPM&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">discrete_returns</span> <span class="o">-</span> <span class="n">discrete_returns</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">/</span><span class="n">discrete_returns</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="n">V</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pca_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">))]</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">cumsum</span><span class="p">())</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cumulative explained variance ratio&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">V</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">V</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;PCA score 1&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;PCA score 2&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e4231b0d2a66babf8dbbc65c3534ae125f85a07408f3dd089f227a25003e551d.png" src="_images/e4231b0d2a66babf8dbbc65c3534ae125f85a07408f3dd089f227a25003e551d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pcs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">discrete_returns</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;PCA 1&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA 2&quot;</span><span class="p">])</span>
<span class="n">pcs</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>MSFT</th>
      <th>MS</th>
      <th>JPM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>PCA 1</th>
      <td>-0.439</td>
      <td>-0.5150</td>
      <td>-0.5250</td>
      <td>-0.5162</td>
    </tr>
    <tr>
      <th>PCA 2</th>
      <td>0.581</td>
      <td>0.4623</td>
      <td>-0.4718</td>
      <td>-0.4754</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">discrete_returns</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AAPL</th>
      <th>MSFT</th>
      <th>MS</th>
      <th>JPM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AAPL</th>
      <td>1.000000</td>
      <td>0.473253</td>
      <td>0.065260</td>
      <td>0.104119</td>
    </tr>
    <tr>
      <th>MSFT</th>
      <td>0.473253</td>
      <td>1.000000</td>
      <td>0.201407</td>
      <td>0.146745</td>
    </tr>
    <tr>
      <th>MS</th>
      <td>0.065260</td>
      <td>0.201407</td>
      <td>1.000000</td>
      <td>0.512140</td>
    </tr>
    <tr>
      <th>JPM</th>
      <td>0.104119</td>
      <td>0.146745</td>
      <td>0.512140</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">discrete_returns</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AAPL    1.642631
MSFT    1.821404
MS      1.778807
JPM     1.763004
dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="k-means-clustering">
<h3>K-means clustering<a class="headerlink" href="#k-means-clustering" title="Link to this heading">#</a></h3>
<p>Clustering involves dividing the data into subgroups, where members of each group should be as similar as possible to their own group members and as different as possible from members of other groups. The similarity of pairwise observations is evaluated using all variables. Similarity measures can be chosen, popular examples are euclidean or cosine distances. Depending which measure is used, one needs to standardize the data (especially when using euclidean distances).</p>
<p>For K-means clustering, the number <span class="math notranslate nohighlight">\(K\)</span> of clusters is first set by the user. The cluster allocation <span class="math notranslate nohighlight">\(C_k\)</span> for cluster <span class="math notranslate nohighlight">\(k\)</span> is the set of data points in the corresponding cluster. For example, if <span class="math notranslate nohighlight">\(10\)</span> data points are contained in a dataset, an allocation for <span class="math notranslate nohighlight">\(k = 3\)</span> could be like:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
C_1 = &amp; \lbrace \boldsymbol{x}_4, \boldsymbol{x}_6, \boldsymbol{x}_8, \boldsymbol{x}_{10} \rbrace \\
C_2 = &amp; \lbrace \boldsymbol{x}_2, \boldsymbol{x}_5, \boldsymbol{x}_7 \rbrace \\
C_3 = &amp; \lbrace \boldsymbol{x}_1, \boldsymbol{x}_3, \boldsymbol{x}_9 \rbrace \\
\end{align}
\end{split}\]</div>
<p>The aim of the algorithm is to perform this allocation in such a way that the average sum of the pairwise distances within the respective clusters is as small as possible. The squared Euclidean distance is used for quantification. Within a cluster, the average sum of all pairwise squared Euclidean distances is defined by:</p>
<div class="math notranslate nohighlight">
\[
W \left( C_k \right) = \frac{1}{|C_k|} \sum_{i, l \in C_k} \sum_{j = 1}^p \left( x_{ij} - x_{lj} \right)^2
\]</div>
<p><span class="math notranslate nohighlight">\(|C_k|\)</span> is the set of all observations in cluster <span class="math notranslate nohighlight">\(k\)</span>. Using this definition, we can obtain the loss function of K-means clustering by:</p>
<div class="math notranslate nohighlight">
\[
L\left( C_1, ..., C_k, X \right) = \sum_{k = 1}^K W \left( C_k \right) = \sum_{k = 1}^K \frac{1}{|C_k|} \sum_{i, l \in C_k} \sum_{j = 1}^p \left( x_{ij} - x_{lj} \right)^2
\]</div>
<p>Since there is no analytical solution for the minimization of the loss function, the loss function is minimized by iterative procedure. The K-means algorithm proceeds as follows:</p>
<ol class="arabic simple">
<li><p>Each observation is randomly assigned a cluster at the beginning.</p></li>
<li><p>Perform the following steps until the cluster assignments do not change.</p>
<ol class="arabic simple">
<li><p>determine the centroid for each cluster. The cluster centroid of the cluster <span class="math notranslate nohighlight">\(k\)</span> is the average vector of the respective features for all observations within the cluster.</p></li>
<li><p>assign the observations to the cluster with the smallest distance to the centroid.</p></li>
</ol>
</li>
</ol>
<p>In the following cells, we run the algorithm over two iterations. We can see well that after the random initialization, matching clusters are found relatively quickly and the cluster centroids move further apart. A critical aspect in practice is the sensitivity of the algorithm to the random assignment performed at the beginning. In addition, the clusters should be characterized and compared by their respective centroids. For example, if the algorithm runs multiple times, the cluster with the same centroid can be named cluster <span class="math notranslate nohighlight">\(C_0\)</span> once and cluster <span class="math notranslate nohighlight">\(C_2\)</span> the next time. This assignment is arbitrary, but the centroids found are characteristic. In addition, it is important to know that the data should be normalized before clustering, otherwise the Euclidean distance or squared Euclidean distance is more influenced by the variable with the higher numerical values. For example, if the values of two features are in the range <span class="math notranslate nohighlight">\([0, 1]\)</span> and <span class="math notranslate nohighlight">\([0, 1000]\)</span>, a distance of <span class="math notranslate nohighlight">\(10^2\)</span> of the second feature would dominate the quantification of the Euclidean distance compared to the maximum distance <span class="math notranslate nohighlight">\(1^2\)</span> of the first feature, although this should probably be considered relatively small in relation to the total numerical range.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>


<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#1f77b4&quot;</span><span class="p">,</span> <span class="s2">&quot;#ff7f0e&quot;</span><span class="p">,</span> <span class="s2">&quot;#2ca02c&quot;</span><span class="p">]</span>
<span class="n">X</span><span class="p">,</span> <span class="n">clusters</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x_1&quot;</span><span class="p">,</span> <span class="s2">&quot;x_2&quot;</span><span class="p">])</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;est_cluster&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">k</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">cluster_centroids</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">est_cluster</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
        <span class="n">df_tmp</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">est_cluster</span> <span class="o">==</span> <span class="n">cluster</span><span class="p">]</span>
        <span class="n">df_tmp</span> <span class="o">=</span> <span class="n">df_tmp</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;est_cluster&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">cluster_centroids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df_tmp</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">cluster_centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cluster_centroids</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">est_cluster</span><span class="o">.</span><span class="n">unique</span><span class="p">()):</span>
        <span class="n">df_tmp</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">est_cluster</span> <span class="o">==</span> <span class="n">cluster</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_tmp</span><span class="o">.</span><span class="n">x_1</span><span class="p">,</span> <span class="n">df_tmp</span><span class="o">.</span><span class="n">x_2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Cluster </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.60</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">cluster_centroids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cluster_centroids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Data with cluster assignment&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;est_cluster&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">euclidean_distances</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;est_cluster&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">cluster_centroids</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9a1ff4db3fce1618cc142ea578ac5c86bebedb12025f5e86c487037a24044a6c.png" src="_images/9a1ff4db3fce1618cc142ea578ac5c86bebedb12025f5e86c487037a24044a6c.png" />
<img alt="_images/39735ba83a5e24988266e24a199f9d79e6164f86aa146877fa91cff1f4e2989f.png" src="_images/39735ba83a5e24988266e24a199f9d79e6164f86aa146877fa91cff1f4e2989f.png" />
<img alt="_images/c1de20036f3f40e64ec081c93253f2ce5b7715ebd307f971f955da82dee0d3d2.png" src="_images/c1de20036f3f40e64ec081c93253f2ce5b7715ebd307f971f955da82dee0d3d2.png" />
</div>
</div>
<p>After cluster allocation, the K-means algorithm has to find the best value for <span class="math notranslate nohighlight">\(K\)</span>. For this purpose, the quality of the allocation found in each case must be quantified and compared with the other allocations. An example for a corresponding measure is the silhouette score of an observation. Here, on the one hand, the average squared Euclidean distance of the <span class="math notranslate nohighlight">\(i\)</span>-th observation to all other observations within the cluster of <span class="math notranslate nohighlight">\(i\)</span> is determined.</p>
<div class="math notranslate nohighlight">
\[
a(i) = \frac{1}{| C_k| - 1} \sum_{l \in C_k, l \neq i} \sum_{j = 1}^p \left( x_{ij} - x_{lj} \right)^2, i \in C_k
\]</div>
<p>In addition, the average squared Euclidean distance of the <span class="math notranslate nohighlight">\(i\)</span>th observation to the nearest cluster is determined:</p>
<div class="math notranslate nohighlight">
\[
b(i) = \min_{C_q} \frac{1}{| C_q| - 1} \sum_{l \in C_q, i \neq l} \sum_{j = 1}^p \left( x_{ij} - x_{lj} \right)^2, i \not\in C_q
\]</div>
<p>The silhouette score for the <span class="math notranslate nohighlight">\(i\)</span>-th observation is given by:</p>
<div class="math notranslate nohighlight">
\[
S(i) = \frac{b(i) - a(i)}{\max \lbrace a(i), b(i)\rbrace}
\]</div>
<p>The value is in the interval <span class="math notranslate nohighlight">\([-1, 1]\)</span>, where a value close to <span class="math notranslate nohighlight">\(1\)</span> signals very good separation of the observation by its cluster assignment, while a negative value suggests that this observation is on average closer to the observations of the nearest cluster. Averages over a cluster can be used as a cluster-specific metric, while the average over all observations can be used as a metric for the entire cluster classification. In the left plot of the next graph, we see the average silhouette score over all clusters for a different number of clusters. We can see that the actual number of clusters <span class="math notranslate nohighlight">\(K=3\)</span> would also be identified using this metric. The estimated cluster assignments for <span class="math notranslate nohighlight">\(K=3\)</span> can be seen in the right plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_samples</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="n">X</span><span class="p">,</span> <span class="n">clusters</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x_1&quot;</span><span class="p">,</span> <span class="s2">&quot;x_2&quot;</span><span class="p">])</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">max_clusters</span> <span class="o">=</span> <span class="mi">7</span>
<span class="k">for</span> <span class="n">n_cluster</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_clusters</span><span class="p">):</span> 
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">n_cluster</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">silhouette_scores</span> <span class="o">=</span> <span class="n">silhouette_samples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s2">&quot;euclidean&quot;</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">silhouette_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">,</span> <span class="s2">&quot;4&quot;</span><span class="p">,</span> <span class="s2">&quot;5&quot;</span><span class="p">,</span> <span class="s2">&quot;6&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of clusters&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Average silhouette score&quot;</span><span class="p">)</span>


<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;cluster&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">cluster</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;x_1&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;x_2&quot;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Cluster </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Estimated cluster assignments for $K=3$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5c3c3e67f990bb7b7520885b2eaab479405710ac2330b89e123720eb17d07cf1.png" src="_images/5c3c3e67f990bb7b7520885b2eaab479405710ac2330b89e123720eb17d07cf1.png" />
</div>
</div>
<p>An even more differentiated graphical view of the clustering quality can be generated if we visualize the silhouette scores in ascending order for each estimated cluster. In this way, we can see in which clusters the highest scores occur and in which clusters, on the other hand, individual less good assignments take place. We also look at the appropriate graph for our example and see that the cluster in the lower left corner has the highest silhouette scores. This makes sense, since it is further away from the other two clusters, which results in a better delineation. We also see that a negative silhouette score is realized for the cluster in the middle. This is the single data point that is very close to the cluster in the left corner. The idea of the Silhouette Score can of course be adapted for other distance measures. In addition, other metrics can be used to evaluate the cluster assignment. However, the operation of most metrics is similar in principle to the Silhouette Score, which is why we refrain from further illustrations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">distances</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">silhouette_samples</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;silhouette_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">silhouette_samples</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">cluster</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;silhouette_scores&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;silhouette_scores&quot;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">legend</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Cluster </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">xticks</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_xticks</span><span class="p">()</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;&quot;</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xticks</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Silhouette scores&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7c04be1a685f777f2d0ad86ed72411ec061549ca13d63e5e6436437e565c43bc.png" src="_images/7c04be1a685f777f2d0ad86ed72411ec061549ca13d63e5e6436437e565c43bc.png" />
</div>
</div>
<p>K-means clustering also has some potential drawbacks, e.g., every observation must be included in one cluster. This can be problematic as data sets may include outlier observations which do not share large similarities to any of the other observations. Examples for algorithms which include a category for outlier categories are the DBSCAN or HDBSCAN algorithm.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_dependence_matters.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Multivariate analysis - dependence matters!</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-between-machine-learning-and-traditional-statistical-analysis">Differences Between Machine Learning and Traditional Statistical Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-tasks">Machine learning tasks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">Supervised learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression">Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification">Binary classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-classification">Multi-classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-and-generalization-of-supervised-learning-models">Validation and generalization of supervised learning models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning">Unsupervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis">Principal component analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering">K-means clustering</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>